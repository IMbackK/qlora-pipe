# Paths
model = '/data/models/Mistral-7B-v0.1'
output_dir = '/home/anon/training_runs/mistral_7b_alpaca'

# Dataset configuration
dataset_type = 'axolotl'
dataset_path = 'examples/alpaca.yml'
# set a cache directory if you're using 'doclist' or 'textfile' dataset types
# (for axolotl, it's unneeded because axolotl code automatically caches everything behind the scenes)
#dataset_cache_dir = "/home/anon/data/cache/something"
sequence_len = 2048
eval_size = 0.01

# Instead of using eval_size which splits off some of dataset, we can have a completely separate dataset for eval.
# This can be useful if you're training on raw text data, so that the eval set remains completely fixed, even if
# you change training sequence_len, etc.
# eval_dataset_path = "/home/anon/data/my_dataset/eval_data.jsonl"
# eval_dataset_type = 'doclist'
# eval_dataset_cache_dir = "/home/anon/data/cache/my_dataset_eval"
# eval_sequence_len = 4096

# Lora configuration
# can use full_fine_tune=true and load_in_4bit=false to train the whole model instead of a LoRA
#full_fine_tune = true
load_in_4bit = true
lora_rank = 64
lora_alpha = 64
lora_dropout = 0.05
target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
# can specify layers to adapt with LoRA if you want
#layers_to_transform = '16:31'

# Optimization configuration
epochs = 2
lr_scheduler = 'cosine'
warmup_steps = 100

# might be useful if resuming from a checkpoint and you want to change the LR and force it to something
#force_constant_lr = 5e-5

# hard clamp the magnitude of the LoRA weights
#scale_weight_norms = 1.0

# dynamic batch size, targeting this many tokens per batch, per device
# if set, completely ignores the batch size in the deepspeed JSON config file
# can be thought of as a replacement for sample packing
batch_size_tokens = 10000

# Performance settings
pipeline_stages = 1  # number of pipeline parallel stages, must evenly divide the number of GPUs you launch the script with
logging_steps = 10  # how often to log in Tensorboard
eval_steps = 100
save_steps = 200
checkpoint_every_n_minutes = 60
eval_before_first_step = true  # do an eval before any training happens
bnb_compute_dtype = 'bfloat16'
lora_weight_dtype = 'bfloat16'
use_double_quant = false

# sort examples by length before dividing them into batches
# this makes all examples in a batch approximately the same length, to minimize padding
# the batches are still shuffled after that
# you should probably always have this set to true
group_by_length = true

activation_checkpointing = true
# experimental feature, doesn't currently work
#offload_mlp_to_cpu = false

# Resume a prior run
# if true, we attempt to resume training from the most recent directory inside output_dir (the directory names are timestamps)
resume_from_checkpoint = false
